=====================================================================
使用新仿真器 (HDF5) 数据训练损伤检测模型
======================================================================

[步骤1] 加载仿真数据...
[Optimization] 正在构建原始信号索引...
[Optimization] 索引构建完成，总样本数: 10000

✓ 样本数 (10000) 足够进行训练

[步骤2] 初始化检测系统...
  ✓ 设备: cpu
  ✓ 类别数: 2
  ✓ 模型参数量: 3,241,826

[步骤3] 划分数据集 (按Scenario)...
  ✓ 训练集场景数: 1500, 样本数: 6000
  ✓ 验证集场景数: 500, 样本数: 2000
  ✓ 测试集场景数: 500, 样本数: 2000
[Optimization] 正在构建原始信号索引...
[Optimization] 索引构建完成，总样本数: 10000
[Optimization] 正在构建原始信号索引...
[Optimization] 索引构建完成，总样本数: 10000
[Optimization] 正在构建原始信号索引...
[Optimization] 索引构建完成，总样本数: 10000

[步骤4] 创建 DataLoader...

[步骤5] 训练模型...
  训练轮数: 30
  学习率: 0.001
  早停耐心值: 8
Batch 20/188 Loss: 0.7851 TS Grad: 2.0913 Img Grad: 2.3894 Ratio: 1.14
Batch 40/188 Loss: 0.6161 TS Grad: 1.6839 Img Grad: 1.2377 Ratio: 0.74
Batch 60/188 Loss: 0.7285 TS Grad: 2.2813 Img Grad: 1.6530 Ratio: 0.72
Batch 80/188 Loss: 0.4676 TS Grad: 1.6761 Img Grad: 0.3742 Ratio: 0.22
Batch 100/188 Loss: 0.5708 TS Grad: 2.2729 Img Grad: 0.4478 Ratio: 0.20
Batch 120/188 Loss: 0.4517 TS Grad: 1.6705 Img Grad: 0.4893 Ratio: 0.29
Batch 140/188 Loss: 0.5241 TS Grad: 2.1189 Img Grad: 0.6863 Ratio: 0.32
Batch 160/188 Loss: 0.4532 TS Grad: 1.7267 Img Grad: 0.9585 Ratio: 0.56
Batch 180/188 Loss: 0.5242 TS Grad: 2.2892 Img Grad: 1.0542 Ratio: 0.46

Epoch [1/30], Train Loss: 0.5618, Acc: 70.22%, Val Loss: 0.4764, Acc: 77.60%

Batch 20/188 Loss: 0.5407 TS Grad: 2.3859 Img Grad: 0.3939 Ratio: 0.17
Batch 40/188 Loss: 0.5653 TS Grad: 2.6683 Img Grad: 0.4376 Ratio: 0.16
Batch 60/188 Loss: 0.5065 TS Grad: 2.2778 Img Grad: 0.7124 Ratio: 0.31
Batch 80/188 Loss: 0.5650 TS Grad: 2.6442 Img Grad: 0.4764 Ratio: 0.18
Batch 100/188 Loss: 0.4558 TS Grad: 2.0900 Img Grad: 0.7717 Ratio: 0.37
Batch 120/188 Loss: 0.4534 TS Grad: 2.6450 Img Grad: 0.6793 Ratio: 0.26
Batch 140/188 Loss: 0.5168 TS Grad: 3.8017 Img Grad: 0.9516 Ratio: 0.25
Batch 160/188 Loss: 0.4104 TS Grad: 2.8109 Img Grad: 0.6324 Ratio: 0.22
Batch 180/188 Loss: 0.5183 TS Grad: 2.5773 Img Grad: 0.6346 Ratio: 0.25

Epoch [2/30], Train Loss: 0.4501, Acc: 80.48%, Val Loss: 0.4548, Acc: 80.30%

Batch 20/188 Loss: 0.3131 TS Grad: 1.7237 Img Grad: 0.4975 Ratio: 0.29
Batch 40/188 Loss: 0.4924 TS Grad: 4.1705 Img Grad: 2.4655 Ratio: 0.59
Batch 60/188 Loss: 0.2971 TS Grad: 1.6810 Img Grad: 1.2656 Ratio: 0.75
Batch 80/188 Loss: 0.3140 TS Grad: 2.1308 Img Grad: 0.9124 Ratio: 0.43
Batch 100/188 Loss: 0.2392 TS Grad: 0.9950 Img Grad: 0.4958 Ratio: 0.50
Batch 120/188 Loss: 0.5377 TS Grad: 2.7145 Img Grad: 1.7540 Ratio: 0.65
Batch 140/188 Loss: 0.3330 TS Grad: 1.8885 Img Grad: 1.0523 Ratio: 0.56
Batch 160/188 Loss: 0.6813 TS Grad: 3.1108 Img Grad: 3.2418 Ratio: 1.04
Batch 180/188 Loss: 0.3284 TS Grad: 2.1907 Img Grad: 0.9465 Ratio: 0.43

Epoch [3/30], Train Loss: 0.3917, Acc: 84.35%, Val Loss: 0.5150, Acc: 72.25%

Batch 20/188 Loss: 0.2550 TS Grad: 1.5563 Img Grad: 1.2121 Ratio: 0.78
Batch 40/188 Loss: 0.3987 TS Grad: 1.9805 Img Grad: 2.2558 Ratio: 1.14
Batch 60/188 Loss: 0.2860 TS Grad: 2.0728 Img Grad: 1.7006 Ratio: 0.82
Batch 80/188 Loss: 0.2509 TS Grad: 1.1557 Img Grad: 0.5233 Ratio: 0.45
Batch 100/188 Loss: 0.3016 TS Grad: 1.7932 Img Grad: 1.1004 Ratio: 0.61
Batch 120/188 Loss: 0.2568 TS Grad: 1.9383 Img Grad: 1.7799 Ratio: 0.92
Batch 140/188 Loss: 0.3216 TS Grad: 3.3132 Img Grad: 2.0482 Ratio: 0.62
Batch 160/188 Loss: 0.4100 TS Grad: 2.3792 Img Grad: 0.9123 Ratio: 0.38
Batch 180/188 Loss: 0.2148 TS Grad: 1.9493 Img Grad: 0.6110 Ratio: 0.31

Epoch [4/30], Train Loss: 0.3267, Acc: 88.05%, Val Loss: 0.4086, Acc: 82.90%

Batch 20/188 Loss: 0.2569 TS Grad: 1.9274 Img Grad: 1.1804 Ratio: 0.61
Batch 40/188 Loss: 0.1723 TS Grad: 1.0550 Img Grad: 0.9485 Ratio: 0.90
Batch 60/188 Loss: 0.2971 TS Grad: 1.9615 Img Grad: 1.1497 Ratio: 0.59
Batch 80/188 Loss: 0.2862 TS Grad: 2.0984 Img Grad: 1.5594 Ratio: 0.74
Batch 100/188 Loss: 0.2152 TS Grad: 1.8480 Img Grad: 2.4378 Ratio: 1.32
Batch 120/188 Loss: 0.1709 TS Grad: 1.2038 Img Grad: 0.6296 Ratio: 0.52
Batch 140/188 Loss: 0.2100 TS Grad: 1.1970 Img Grad: 0.8761 Ratio: 0.73
Batch 160/188 Loss: 0.2073 TS Grad: 2.1470 Img Grad: 1.6847 Ratio: 0.78
Batch 180/188 Loss: 0.2242 TS Grad: 2.3953 Img Grad: 1.2114 Ratio: 0.51

Epoch [5/30], Train Loss: 0.2733, Acc: 91.23%, Val Loss: 0.5915, Acc: 76.70%

Batch 20/188 Loss: 0.2177 TS Grad: 2.3933 Img Grad: 2.5414 Ratio: 1.06
Batch 40/188 Loss: 0.2142 TS Grad: 2.1769 Img Grad: 2.8394 Ratio: 1.30
Batch 60/188 Loss: 0.1728 TS Grad: 2.1925 Img Grad: 1.9705 Ratio: 0.90
Batch 80/188 Loss: 0.1774 TS Grad: 1.5030 Img Grad: 2.0322 Ratio: 1.35
Batch 100/188 Loss: 0.3297 TS Grad: 3.3855 Img Grad: 6.3284 Ratio: 1.87
Batch 120/188 Loss: 0.1538 TS Grad: 1.2778 Img Grad: 2.2504 Ratio: 1.76
Batch 140/188 Loss: 0.2568 TS Grad: 2.1033 Img Grad: 2.0733 Ratio: 0.99
Batch 160/188 Loss: 0.1455 TS Grad: 1.0558 Img Grad: 0.8685 Ratio: 0.82
Batch 180/188 Loss: 0.2109 TS Grad: 3.5953 Img Grad: 1.7018 Ratio: 0.47

Epoch [6/30], Train Loss: 0.2276, Acc: 93.43%, Val Loss: 0.4899, Acc: 81.50%

Batch 20/188 Loss: 0.1523 TS Grad: 1.6675 Img Grad: 1.2572 Ratio: 0.75
Batch 40/188 Loss: 0.1935 TS Grad: 2.4428 Img Grad: 1.8233 Ratio: 0.75
Batch 60/188 Loss: 0.1121 TS Grad: 0.5564 Img Grad: 0.4222 Ratio: 0.76
Batch 80/188 Loss: 0.1087 TS Grad: 0.4266 Img Grad: 0.7312 Ratio: 1.71
Batch 100/188 Loss: 0.2422 TS Grad: 1.8018 Img Grad: 4.0954 Ratio: 2.27
Batch 120/188 Loss: 0.1201 TS Grad: 0.9917 Img Grad: 1.5336 Ratio: 1.55
Batch 140/188 Loss: 0.2066 TS Grad: 2.2808 Img Grad: 2.3733 Ratio: 1.04
Batch 160/188 Loss: 0.2148 TS Grad: 1.0774 Img Grad: 1.0397 Ratio: 0.97
Batch 180/188 Loss: 0.2057 TS Grad: 1.7496 Img Grad: 1.5379 Ratio: 0.88

Epoch [7/30], Train Loss: 0.1918, Acc: 95.23%, Val Loss: 0.4961, Acc: 81.80%

Batch 20/188 Loss: 0.2089 TS Grad: 2.3490 Img Grad: 1.8418 Ratio: 0.78
Batch 40/188 Loss: 0.2417 TS Grad: 2.8395 Img Grad: 2.0559 Ratio: 0.72
Batch 60/188 Loss: 0.1251 TS Grad: 1.2061 Img Grad: 1.1099 Ratio: 0.92
Batch 80/188 Loss: 0.1581 TS Grad: 1.8842 Img Grad: 2.9618 Ratio: 1.57
Batch 100/188 Loss: 0.1761 TS Grad: 2.7624 Img Grad: 7.5427 Ratio: 2.73
Batch 120/188 Loss: 0.1825 TS Grad: 1.0496 Img Grad: 2.1733 Ratio: 2.07
Batch 140/188 Loss: 0.1958 TS Grad: 1.2916 Img Grad: 1.7032 Ratio: 1.32
Batch 160/188 Loss: 0.1385 TS Grad: 0.8505 Img Grad: 0.6576 Ratio: 0.77
Batch 180/188 Loss: 0.2922 TS Grad: 1.9994 Img Grad: 1.7460 Ratio: 0.87

Epoch [8/30], Train Loss: 0.1735, Acc: 96.23%, Val Loss: 0.5049, Acc: 83.60%

Batch 20/188 Loss: 0.1879 TS Grad: 3.2500 Img Grad: 2.3890 Ratio: 0.74
Batch 40/188 Loss: 0.1293 TS Grad: 1.4020 Img Grad: 3.6836 Ratio: 2.63
Batch 60/188 Loss: 0.1178 TS Grad: 0.9318 Img Grad: 4.1093 Ratio: 4.41
Batch 80/188 Loss: 0.1047 TS Grad: 0.4163 Img Grad: 0.5347 Ratio: 1.28
Batch 100/188 Loss: 0.1428 TS Grad: 0.5315 Img Grad: 0.6357 Ratio: 1.20
Batch 120/188 Loss: 0.1020 TS Grad: 0.5305 Img Grad: 0.4867 Ratio: 0.92
Batch 140/188 Loss: 0.0928 TS Grad: 0.1328 Img Grad: 0.1263 Ratio: 0.95
Batch 160/188 Loss: 0.1808 TS Grad: 1.1700 Img Grad: 0.8750 Ratio: 0.75
Batch 180/188 Loss: 0.2108 TS Grad: 2.5420 Img Grad: 3.1510 Ratio: 1.24

Epoch [9/30], Train Loss: 0.1597, Acc: 97.07%, Val Loss: 0.9074, Acc: 69.40%

Batch 20/188 Loss: 0.0992 TS Grad: 0.3097 Img Grad: 0.2345 Ratio: 0.76
Batch 40/188 Loss: 0.1620 TS Grad: 2.9468 Img Grad: 1.4056 Ratio: 0.48
Batch 60/188 Loss: 0.1016 TS Grad: 0.7233 Img Grad: 1.6639 Ratio: 2.30
Batch 80/188 Loss: 0.1030 TS Grad: 0.6732 Img Grad: 0.5004 Ratio: 0.74
Batch 100/188 Loss: 0.1258 TS Grad: 1.6997 Img Grad: 1.1772 Ratio: 0.69
Batch 120/188 Loss: 0.1183 TS Grad: 0.6824 Img Grad: 0.5584 Ratio: 0.82
Batch 140/188 Loss: 0.1136 TS Grad: 1.4340 Img Grad: 1.7842 Ratio: 1.24
Batch 160/188 Loss: 0.1187 TS Grad: 2.0329 Img Grad: 1.2994 Ratio: 0.64
Batch 180/188 Loss: 0.0949 TS Grad: 0.2148 Img Grad: 0.2826 Ratio: 1.32

Epoch [10/30], Train Loss: 0.1484, Acc: 97.27%, Val Loss: 0.5220, Acc: 83.10%

Batch 20/188 Loss: 0.0980 TS Grad: 0.7172 Img Grad: 0.4876 Ratio: 0.68
Batch 40/188 Loss: 0.1584 TS Grad: 1.6694 Img Grad: 1.3883 Ratio: 0.83
Batch 60/188 Loss: 0.1397 TS Grad: 2.0379 Img Grad: 2.3860 Ratio: 1.17
Batch 80/188 Loss: 0.1436 TS Grad: 2.3244 Img Grad: 2.0278 Ratio: 0.87
Batch 100/188 Loss: 0.1120 TS Grad: 1.2741 Img Grad: 0.4786 Ratio: 0.38
Batch 120/188 Loss: 0.0925 TS Grad: 0.2421 Img Grad: 0.4043 Ratio: 1.67
Batch 140/188 Loss: 0.0848 TS Grad: 0.0784 Img Grad: 0.0481 Ratio: 0.61
Batch 160/188 Loss: 0.1017 TS Grad: 0.1736 Img Grad: 0.1582 Ratio: 0.91
Batch 180/188 Loss: 0.0977 TS Grad: 0.1377 Img Grad: 0.1674 Ratio: 1.22

Epoch [11/30], Train Loss: 0.1257, Acc: 98.60%, Val Loss: 0.5138, Acc: 82.10%

Batch 20/188 Loss: 0.0905 TS Grad: 0.2195 Img Grad: 0.1348 Ratio: 0.61
Batch 40/188 Loss: 0.0923 TS Grad: 0.1300 Img Grad: 0.1424 Ratio: 1.10
Batch 60/188 Loss: 0.0920 TS Grad: 0.2176 Img Grad: 0.3480 Ratio: 1.60
Batch 80/188 Loss: 0.1030 TS Grad: 1.0607 Img Grad: 0.5333 Ratio: 0.50
Batch 100/188 Loss: 0.0886 TS Grad: 0.0784 Img Grad: 0.0536 Ratio: 0.68
Batch 120/188 Loss: 0.1017 TS Grad: 0.4269 Img Grad: 0.4213 Ratio: 0.99
Batch 140/188 Loss: 0.1740 TS Grad: 1.2351 Img Grad: 2.4311 Ratio: 1.97
Batch 160/188 Loss: 0.0960 TS Grad: 0.0970 Img Grad: 0.0643 Ratio: 0.66
Batch 180/188 Loss: 0.1496 TS Grad: 2.5781 Img Grad: 4.1511 Ratio: 1.61
Early stopping at epoch 12

[步骤5] 评估模型性能...

==================================================
测试集性能:
==================================================
准确率:   0.8520
精确率:   0.8450
召回率:   0.8544
F1分数:   0.8482

============================================================
开始进行消融实验
============================================================
模型配置                 | Loss       | Accuracy   | 贡献度       
------------------------------------------------------------
Full Model           | 0.4182     | 85.20     % | (Baseline)
Image Only           | 0.5257     | 76.55     % | (-8.65%)
Time Series Only     | 0.5067     | 76.90     % | (-8.30%)
------------------------------------------------------------
✓ 结论: 模型有效融合了两个模态的信息，融合效果优于单模态。
============================================================

[步骤6] 保存模型...
  ✓ 模型已保存: new_simulator_trained_model.pth

======================================================================
✓ 训练完成！
======================================================================